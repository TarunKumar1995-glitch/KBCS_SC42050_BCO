{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,) 2\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "render=True\n",
    "env = gym.make('CartPole-v0')\n",
    "print(env.observation_space.shape, env.action_space.n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BCO(nn.Module):\n",
    "    def __init__(self, env, policy='mlp'):\n",
    "        super(BCO, self).__init__()\n",
    "        \n",
    "        self.policy = policy\n",
    "        self.act_n = env.action_space.n\n",
    "        \n",
    "        if self.policy=='mlp':\n",
    "            self.obs_n = env.observation_space.shape[0]\n",
    "            self.pol = nn.Sequential(*[nn.Linear(self.obs_n, 32),\n",
    "                                       nn.Linear(32, self.act_n)])\n",
    "            self.inv = nn.Sequential(*[nn.Linear(self.obs_n*2, 32),\n",
    "                                       nn.Linear(32, self.act_n)])\n",
    "        \n",
    "        elif self.policy=='cnn':\n",
    "            pass\n",
    "    \n",
    "    def pred_act(self, obs):\n",
    "        out = self.pol(obs)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def pred_inv(self, obs1, obs2):\n",
    "        obs = T.cat([obs1, obs2], dim=1)\n",
    "        out = self.inv(obs)\n",
    "        \n",
    "        return out\n",
    "\n",
    "POLICY = 'mlp'\n",
    "model = BCO(env, policy=POLICY).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class DS_Inv(Dataset):\n",
    "    def __init__(self, trajs):\n",
    "        self.dat = []\n",
    "        \n",
    "        for traj in trajs:\n",
    "            for dat in traj:\n",
    "                obs, act, new_obs = dat\n",
    "                \n",
    "                self.dat.append([obs, new_obs, act])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dat)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        obs, new_obs, act = self.dat[idx]\n",
    "        \n",
    "        return obs, new_obs, np.asarray(act)\n",
    "\n",
    "class DS_Policy(Dataset):\n",
    "    def __init__(self, traj):\n",
    "        self.dat = []\n",
    "        \n",
    "        for dat in traj:\n",
    "            obs, act = dat\n",
    "                \n",
    "            self.dat.append([obs, act])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dat)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        obs, act = self.dat[idx]\n",
    "        \n",
    "        return obs, np.asarray(act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 4]) torch.Size([100]) torch.Size([100, 4])\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "trajs_demo = pickle.load(open('Demo/demo_cart-pole.pkl', 'rb'))\n",
    "ld_demo = DataLoader(DS_Inv(trajs_demo), batch_size=100)\n",
    "\n",
    "for obs1, obs2, act  in ld_demo:\n",
    "    print(obs1.shape,act.shape, obs2.shape)\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss().cuda()\n",
    "optim = T.optim.Adam(model.parameters(), lr=5e-3)\n",
    "\n",
    "alpha = 0\n",
    "M = 1000\n",
    "\n",
    "EPS = 0.9\n",
    "DECAY = 0.5\n",
    "random_seed = 42\n",
    "epochs = 20\n",
    "patience = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid_loader(dataset, batch_size, validation_split, shuffle_dataset):\n",
    "    dataset_size = len(dataset)\n",
    "    indices = list(range(dataset_size))\n",
    "    split = int(np.floor(validation_split * dataset_size))\n",
    "    if shuffle_dataset :\n",
    "        np.random.seed(random_seed)\n",
    "        np.random.shuffle(indices)\n",
    "    train_indices, val_indices = indices[split:], indices[:split]\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    valid_sampler = SubsetRandomSampler(val_indices)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset,batch_size=batch_size,\n",
    "                                               sampler=train_sampler)\n",
    "    validation_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                                    sampler=valid_sampler)\n",
    "    return train_loader, validation_loader\n",
    "\n",
    "def train_NN(train_loader, NN):\n",
    "    \n",
    "    with tqdm(train_loader, desc='Training',  disable = True, position=0, leave=True) as TQ:\n",
    "        ls_ep = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        if (NN == 'inv'):\n",
    "            for obs1, obs2, act in TQ:\n",
    "                out = model.pred_inv(obs1.float().cuda(), obs2.float().cuda())\n",
    "                out_act = torch.argmax(out.cpu().detach(), axis=1)\n",
    "                ls_bh = loss_func(out, act.cuda())\n",
    "                \n",
    "                optim.zero_grad()\n",
    "                ls_bh.backward()\n",
    "                optim.step()\n",
    "\n",
    "                ls_bh = ls_bh.cpu().detach().numpy()\n",
    "                TQ.set_postfix(loss_policy='%.3f' % (ls_bh))\n",
    "                ls_ep += ls_bh\n",
    "                total += obs1.shape[0]\n",
    "                correct += (out_act == act).sum().item()\n",
    "                \n",
    "        elif(NN == 'pred'):\n",
    "            for obs, act in TQ:\n",
    "                out = model.pred_act(obs.float().cuda())\n",
    "                out_act = torch.argmax(out.cpu().detach(), axis=1)\n",
    "                ls_bh = loss_func(out, act.cuda())\n",
    "\n",
    "                optim.zero_grad()\n",
    "                ls_bh.backward()\n",
    "                optim.step()\n",
    "\n",
    "                ls_bh = ls_bh.cpu().detach().numpy()\n",
    "                TQ.set_postfix(loss_policy='%.3f' % (ls_bh))\n",
    "                ls_ep += ls_bh\n",
    "                total += obs.shape[0]\n",
    "                correct += (out_act == act).sum().item()\n",
    "            \n",
    "        ls_ep /= len(TQ)\n",
    "        accuracy = 100*correct/total\n",
    "        \n",
    "    return ls_ep, accuracy\n",
    "\n",
    "def validate_NN(validation_loader, NN):\n",
    "    \n",
    "    with tqdm(validation_loader, desc='Validate', disable = True, position=0, leave=True) as TQ:\n",
    "        ls_val_ep = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        if (NN == 'inv'):\n",
    "            for obs1, obs2, act in TQ:\n",
    "                out = model.pred_inv(obs1.float().cuda(), obs2.float().cuda())\n",
    "                out_act = torch.argmax(out.cpu().detach(), axis=1)\n",
    "                ls_bh = loss_func(out, act.cuda())\n",
    "                ls_bh = ls_bh.cpu().detach().numpy()\n",
    "                TQ.set_postfix(loss_policy='%.3f' % (ls_bh))\n",
    "                ls_val_ep += ls_bh\n",
    "                total += obs1.shape[0]\n",
    "                correct += (out_act == act).sum().item()\n",
    "        elif (NN == 'pred'):\n",
    "            for obs, act in TQ:\n",
    "                out = model.pred_act(obs.float().cuda())\n",
    "                out_act = torch.argmax(out.cpu().detach(), axis=1)\n",
    "                ls_bh = loss_func(out, act.cuda())\n",
    "                ls_bh = ls_bh.cpu().detach().numpy()\n",
    "                TQ.set_postfix(loss_policy='%.3f' % (ls_bh))\n",
    "                ls_val_ep += ls_bh\n",
    "                total += obs.shape[0]\n",
    "                correct += (out_act == act).sum().item()\n",
    "            \n",
    "        ls_val_ep /= len(TQ)\n",
    "        accuracy = 100*correct/total\n",
    "        \n",
    "        return ls_val_ep, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ID Model Update - Epoch: 7, val loss: 0.011:  35%|███▌      | 7/20 [00:00<00:00, 17.68it/s]\n",
      "Policy Update - Epoch: 17, val loss: 0.477:  85%|████████▌ | 17/20 [00:00<00:00, 18.29it/s]           \n",
      "alpha: 0, Step4: Update Policy: 100%|██████████| 1/1 [00:06<00:00,  6.07s/it]\n"
     ]
    }
   ],
   "source": [
    "trajs_inv = []\n",
    "\n",
    "tqdm_alpha = trange(alpha+1, position=0, desc='alpha:', leave=True)\n",
    "for e in tqdm_alpha:\n",
    "    \n",
    "    # step1, generate inverse samples\n",
    "    tqdm_alpha.set_description(\"alpha: %i, Step1: Exploration\" % e,refresh=True)\n",
    "    time.sleep(1)\n",
    "    cnt = 0 #count\n",
    "    epn = 0 #Episode number\n",
    "    \n",
    "    rews = 0 #Rewards\n",
    "    \n",
    "    while True:\n",
    "        traj = []\n",
    "        rew = 0\n",
    "        N=0 \n",
    "        obs = env.reset()\n",
    "        while True:\n",
    "            inp = T.from_numpy(obs).view(((1, )+obs.shape)).float().cuda()\n",
    "            out = model.pred_act(inp).cpu().detach().numpy()\n",
    "            act = np.argmax(out, axis=1)[0]\n",
    "            if np.random.rand()>=EPS:\n",
    "                act = np.argmax(out, axis=1)[0]\n",
    "            else:\n",
    "                act = env.action_space.sample()\n",
    "        \n",
    "            new_obs, r, done, _ = env.step(act)\n",
    "                \n",
    "            traj.append([obs, act, new_obs])\n",
    "            obs = new_obs\n",
    "            rew += r\n",
    "            \n",
    "            cnt += 1\n",
    "            tqdm_alpha.set_description(\"alpha: %i, Step1: Exploration - %i\" % (e,cnt),refresh=True)\n",
    "            N+=1   \n",
    "            if done==True:\n",
    "                rews += rew\n",
    "                trajs_inv.append(traj)\n",
    "                \n",
    "                epn += 1\n",
    "                \n",
    "                break\n",
    "        \n",
    "        if cnt >= M:\n",
    "            break\n",
    "        \n",
    "    rews /= epn\n",
    "    tqdm_alpha.set_description(\"alpha: %i, step1: Exploration, Reward: %.2f\" % (e,rews),refresh=True)\n",
    "    time.sleep(1)\n",
    "        \n",
    "    # step2, update inverse model\n",
    "    \n",
    "    acc_val_best = 0\n",
    "    patience_cnt = 0\n",
    "    tqdm_alpha.set_description(\"alpha: %i, Step2: Update Inverse Model\" % e,refresh=True)\n",
    "    time.sleep(1)\n",
    "    tqdm_epoch = trange(epochs, position=0, desc='Epoch:', leave=True)\n",
    "    for i in  tqdm_epoch:\n",
    "        dataset=DS_Inv(trajs_inv)\n",
    "        train_loader, validation_loader = train_valid_loader(dataset, batch_size=32, \n",
    "                                                             validation_split=0.3,\n",
    "                                                             shuffle_dataset=True)\n",
    "        \n",
    "        ls_ep, acc_ep = train_NN(train_loader, NN = 'inv')\n",
    "        ls_val_ep, acc_val_ep = validate_NN(validation_loader, NN = 'inv')\n",
    "        \n",
    "        tqdm_epoch.set_description(\"ID Model Update - Epoch: %i, val loss: %.3f\" % (i,ls_val_ep),refresh=True)\n",
    "        \n",
    "        if acc_val_ep > acc_val_best:\n",
    "            acc_val_best = acc_val_ep\n",
    "            patience_cnt = 0\n",
    "    \n",
    "        else:\n",
    "            patience_cnt += 1\n",
    "            if patience_cnt == patience:\n",
    "#                 tqdm.write(\"break\")\n",
    "                break\n",
    "        \n",
    "    \n",
    "    #step3, predict actions\n",
    "    tqdm_alpha.set_description(\"alpha: %i, Step3: Predict most probable actions for expert demos\" % e,refresh=True)\n",
    "    traj_policy = []\n",
    "    \n",
    "    for obs1, obs2, _ in ld_demo:\n",
    "        out = model.pred_inv(obs1.float().cuda(), obs2.float().cuda())\n",
    "        obs = obs1.cpu().detach().numpy()\n",
    "        out = out.cpu().detach().numpy()\n",
    "        out = np.argmax(out, axis=1)\n",
    "        for i in range(100):\n",
    "            traj_policy.append([obs[i], out[i]])\n",
    "\n",
    "    # step4, update policy via demo samples\n",
    "    tqdm_alpha.set_description(\"alpha: %i, Step4: Update Policy\" % e,refresh=True)\n",
    "    acc_val_best = 0\n",
    "    patience_cnt = 0\n",
    "    tqdm_epoch = trange(epochs, position=0, desc='Epochs', leave=True)\n",
    "    for i in  tqdm_epoch:\n",
    "        dataset=DS_Policy(traj_policy)\n",
    "        train_loader, validation_loader = train_valid_loader(dataset, batch_size=32, \n",
    "                                                             validation_split=0.3,\n",
    "                                                             shuffle_dataset=True)\n",
    "        \n",
    "        ls_ep, acc_ep = train_NN(train_loader, NN = 'pred')\n",
    "        ls_val_ep, acc_val_ep = validate_NN(validation_loader, NN = 'pred')\n",
    "        \n",
    "        tqdm_epoch.set_description(\"Policy Update - Epoch: %i, val loss: %.3f\" % (i,ls_val_ep),refresh=True)\n",
    "        \n",
    "        if acc_val_ep > acc_val_best:\n",
    "            acc_val_best = acc_val_ep\n",
    "            patience_cnt = 0\n",
    "    \n",
    "        else:\n",
    "            patience_cnt += 1\n",
    "            if patience_cnt == patience:\n",
    "#                 tqdm.write(\"break\")\n",
    "                break\n",
    "        \n",
    "    time.sleep(1)\n",
    "    # step5, save model\n",
    "    T.save(model.state_dict(), 'Model/model_cart-pole_%d.pt' % (e+1))\n",
    "    \n",
    "    EPS *= DECAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 200 timesteps\n",
      "200.0\n",
      "Episode finished after 200 timesteps\n",
      "200.0\n",
      "Episode finished after 200 timesteps\n",
      "200.0\n",
      "Episode finished after 200 timesteps\n",
      "200.0\n",
      "Episode finished after 200 timesteps\n",
      "200.0\n",
      "Episode finished after 200 timesteps\n",
      "200.0\n",
      "Episode finished after 200 timesteps\n",
      "200.0\n",
      "Episode finished after 200 timesteps\n",
      "200.0\n",
      "Episode finished after 200 timesteps\n",
      "200.0\n",
      "Episode finished after 200 timesteps\n",
      "200.0\n",
      "Episode finished after 200 timesteps\n",
      "200.0\n",
      "Episode finished after 200 timesteps\n",
      "200.0\n",
      "Episode finished after 200 timesteps\n",
      "200.0\n",
      "Episode finished after 200 timesteps\n",
      "200.0\n",
      "Episode finished after 200 timesteps\n",
      "200.0\n",
      "Episode finished after 200 timesteps\n",
      "200.0\n",
      "Episode finished after 200 timesteps\n",
      "200.0\n",
      "Episode finished after 200 timesteps\n",
      "200.0\n",
      "Episode finished after 200 timesteps\n",
      "200.0\n",
      "Episode finished after 200 timesteps\n",
      "200.0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "rews = 0\n",
    "\n",
    "for i_episode in range(20):\n",
    "    observation = env.reset()\n",
    "    rews = 0\n",
    "    for t in range(200):\n",
    "        env.render()\n",
    "        inp = T.from_numpy(observation).view(((1, )+observation.shape)).float().cuda()\n",
    "        out = model.pred_act(inp).cpu().detach().numpy()\n",
    "        act = np.argmax(out, axis=1)[0]  ## Take actions predicted by the inverse dynamics model\n",
    "        observation, reward, done, _ = env.step(act)\n",
    "        rews += reward\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            print(rews)\n",
    "            break\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
